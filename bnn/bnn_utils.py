from emotion_rate import emotion_rating, ethical_scores, ground_truth
import torch
from utils.text_utils import normalize_string, trim_response, extract_choices_and_intro
model = "gpt-4o"
from openai import OpenAI
import pyro

client = OpenAI()
def update_bnn_history(response, agent, bnn_history, max_length, temperature, top_p, global_counter, danger, ethics_score=0, death=False):
    """
    This function captures and stores information about a prompt-response interaction between an agent and the AI.
    It retrieves both the text embeddings and emotional scores for the prompt and response, appending this data to
    the Bayesian Neural Network (BNN) history for future use in decision-making processes.

    Parameters:
    ----------
    response : dict
        The response generated by the AI (typically an OpenAI API response object), which includes both
        the content and metadata for the generated text.

    agent : str
        Identifier for the agent (e.g., "strong" or "weak") which is used to tailor the emotion rating
        for the prompt or response.

    bnn_history : list
        A list that stores the history of interactions, including embeddings, text, and emotional/ethical scores.
        This history can later be used as input to a Bayesian Neural Network (BNN) for decision-making.

    Returns:
    -------
    bnn_history
        The function modifies the `bnn_history` list in place by appending the prompt-response pair along
        with their respective embeddings and emotional/ethical scores.

    Notes:
    ------
    - This function interacts with an emotion rating function `emotion_rating()` to calculate the emotional or
      ethical dimension of the prompt and response based on the agent's perspective.
    - OpenAI's embeddings API is used to generate embeddings for both the prompt and response text.
    - Ensure that `bnn_history` is initialized as an empty list or carries the previous conversation history
      before calling this function.

    Example Usage:
    --------------
    bnn_history = bnn_history(prompt="What should we do next?", response=api_response, first_prompt=True,
                agent="weak", bnn_history=interaction_history)
    """

    if agent in ["Strong", "Weak"]:
        response_embedding = client.embeddings.create(
            input=response,
            model="text-embedding-3-small"
        ).data[0].embedding
        #response_embedding.extend([-1] * 1536 * 4)  ### Commenting out to do single storyteller embedding
        #response_emotion_score = emotion_rating(response, agent, max_length, 0.1, top_p)
        bnn_history.append({
            "id": global_counter,
            "agent": agent,
            "response": response,
            "response_embedding": response_embedding,
            "emotional_and_ethical_score": ethics_score,
            "environment_danger_score": -1,
            "survived": 1
        })
        #print("Response Embedding Length (Agent): ", len(response_embedding))

    elif agent == "Storyteller":
        #intro, choices = extract_choices_and_intro(response) Commenting out to handle single embedding
        #response_embedding = []
        response_embedding = client.embeddings.create(
            input=response,
            model="text-embedding-3-small"
        ).data[0].embedding
        #response_embedding.extend(intro_embedding)
        #print("CHOICE_LIST: ", choices)
        '''
        for choice in choices:
            choice_embedding = client.embeddings.create(
                input=choice,
                model="text-embedding-3-small"
            ).data[0].embedding
            response_embedding.extend(choice_embedding)
        '''
        #print(len(response_embedding))

        if death:
            if len(bnn_history) >= 1:
                bnn_history[-1]["survived"] = 0
                bnn_history[-2]["survived"] = 0
        else:
            environment_danger_score = emotion_rating(response, agent, max_length, 0.1, top_p)
            bnn_history.append({
                "id": global_counter,
                "agent": agent,
                "response": response,
                "response_embedding": response_embedding,
                "emotional_and_ethical_score": -1,
                "environment_danger_score": environment_danger_score,
                "survived": -1, 
                "difficulty": danger
            })
        #print("Response Embedding Length (Storyteller): ", len(response_embedding))

    elif agent == "Power":
        response_embedding = client.embeddings.create(
            input=response,
            model="text-embedding-3-small"
        ).data[0].embedding
        response_embedding.extend([-1] * 1536 * 4)
        bnn_history.append({
                "agent": agent,
                "response": response,
                "response_embedding": response_embedding,
                "emotional_and_ethical_score": 0,
                "environment_danger_score": 0,
                "survived": 1
            })

    #print(bnn_history)

    return bnn_history


def get_activation_function(name):
    activation_functions = {
        'relu': torch.relu,
        'sigmoid': torch.sigmoid,
        'tanh': torch.tanh,
        # Add other activation functions if needed
    }
    return activation_functions.get(name, torch.relu)  # Default to ReLU if not found

def safe_prod(input_tensor, dim=0):
    # Add a small epsilon to avoid zero-product issues
    epsilon = 1e-7
    input_tensor = input_tensor + epsilon
    # Debugging: Check for negative or NaN values
    if torch.any(input_tensor <= 0):
        print("Warning: Non-positive values detected in input_tensor of safe_prod:")
        print(input_tensor)
    if torch.isnan(input_tensor).any():
        print("Warning: NaN values detected in input_tensor of safe_prod:")
        print(input_tensor)
    log_values = torch.log(input_tensor)
    sum_log = torch.sum(log_values, dim=dim)
    result = torch.exp(sum_log)
    return result

def get_aggregation_function(name):
    aggregation_functions = {
        'sum': torch.sum,
        'product': torch.prod,
        'mean': torch.mean,
        'max': torch.max,
        'min': torch.min,
        # Add other aggregation functions if needed
    }
    return aggregation_functions.get(name, torch.sum)  # Default to sum if not found

def get_bnn_state(bnn):
    return {
        "param_store": pyro.get_param_store().get_state(),
        "optim_state": bnn.optimizer.get_state(),
    }

def load_bnn_state(bnn, state_dict):
    pyro.get_param_store().set_state(state_dict["param_store"])
    bnn.optimizer.set_state(state_dict["optim_state"])
