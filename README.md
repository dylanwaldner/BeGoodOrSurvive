# BeGoodOrSurvive

**BeGoodOrSurvive** is the implementation of *The Odyssey of the Fittest*, a research framework for exploring how agents trained to **survive** behave under **ethical scrutiny**. Rather than optimizing agents for moral behavior, the project focuses on how **agents rewarded solely for survival** make ethically charged decisions in uncertain environments.

The core question:  
**Can agents evolve to survive—and still be good?**

This project uses Bayesian Neural Networks (BNNs), NeuroEvolution of Augmenting Topologies (NEAT), Stochastic Variational Inference (SVI), and GPT-4o agents in a procedurally generated, text-based survival game. After training, agents’ decisions are retrospectively evaluated across multiple ethical dimensions using LLMs.

---

## 🎯 Objectives

- Train agents with **self-preservation as their only explicit goal**.
- Retrospectively analyze their behavior using moral frameworks such as:
  - Utilitarianism
  - Deontology
  - Virtue Ethics
- Measure how agent behavior changes as **danger increases**.
- Compare Bayesian and LLM-based approaches to uncertainty and ethics.
- Explore emergent ethical reasoning in large language models (LLMs).

---

## 🧪 Simulation Overview

- **Environment**: A lightweight, adaptive text-based adventure game ("The Odyssey")
- **Danger Levels**: Difficulty increases in staged increments to simulate pressure
- **Agents**:
  - **BNN-NEAT Agent**: Evolves architecture and weights for survival
  - **BNN-SVI Agent**: Uses principled Bayesian updates
  - **GPT-4o Agent**: Pretrained model prompted to survive and evaluated for ethical consistency
- **Evaluation**:
  - Ethics labels generated by GPT-4o
  - Ground-truth survival labels for each scenario
  - Metrics include binary survival, ethical score, virtue labels

---

## 🧠 Architecture Highlights

- **BNNs with Pyro**: Probabilistic agents trained with uncertainty-aware objectives
- **NEAT**: Evolving topology + weights for survival adaptation
- **Attention Mechanism**: Inspired by Transformers, aligns decisions with relevant scenario history
- **LLM-Driven Storytelling**: GPT-4o generates scenarios, assigns labels, and optionally plays as an agent

---

## 📁 Directory Structure

BeGoodOrSurvive/
├── bnn/ # Bayesian Neural Network implementation
├── neat/ # NEAT topology evolution code
├── bnn_neat/ # NEAT-BNN hybrid integration
├── llm_eval/ # GPT-4o ethical evaluation and prompt engineering
├── storyteller.py # Scenario generation using LLM prompts
├── svi_main.py # SVI optimization and training pipeline
├── loops.py # Central game loop and agent training driver
├── ethical_testing.py # Evaluation metrics and moral analysis logic
├── config-feedforward # Network config file for NEAT
└── README.md # This file


---

## 🔍 Key Findings (from paper)

- Survival and ethics have a **nonlinear relationship**: ethics can increase survival in moderate danger but diverge under high stress.
- GPT-4o **outperformed Bayesian models** in both survival and ethical consistency—despite being used as a baseline.
- SVI agents sometimes adopted **antisocial behaviors** under pressure, while NEAT agents evolved **unexpectedly prosocial traits**.
- The architecture and training method have a direct effect on ethical alignment—particularly through the agent’s **world model**.

---

## 🛠️ Getting Started

### 1. Clone the repository
git clone https://github.com/dylanwaldner/BeGoodOrSurvive.git
cd BeGoodOrSurvive

### 2. (Optional) Install dependencies
A requirements.txt file will be added soon
pip install -r requirements.txt

### 3. Run the simulation
python3 main.py # For Neat optimization

#### Or run specific training/evaluation scripts
python3 svi_main.py     # for SVI optimization
python3 ethical_testing.py # for GPT optimization

## 📖 Citation
If you use this codebase, please cite:

@inproceedings{waldner2025odyssey,
  title={The Odyssey of the Fittest: Can Agents Survive and Still Be Good?},
  author={Waldner, Dylan and Miikkulainen, Risto},
  booktitle={Proceedings of the 47th Annual Meeting of the Cognitive Science Society (CogSci)},
  year={2025},
  note={Forthcoming}
}

## 👤 Author  
Dylan Waldner  
AI Ethics Researcher  
The University of Texas at Austin  
[@dylanwaldner](https://github.com/dylanwaldner)  
[https://www.dylanwaldner.com](https://www.dylanwaldner.com)


## 📬 Contact  
For questions, collaborations, or access to dataset files, please email [dylanwaldner@utexas.edu](mailto:dylanwaldner@utexas.edu).








